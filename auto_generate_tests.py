#!/usr/bin/env python3
"""
è‡ªåŠ¨æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
æ ¹æ®ç½‘ç«™æè¿°è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œç„¶åé€ä¸ªæ‰§è¡Œè¯„ä¼°

ä½¿ç”¨æ–¹æ³•:
    python auto_generate_tests.py \
        --url "http://localhost:8000/" \
        --instruction "å®ç°ä¸€ä¸ª2048å°æ¸¸æˆ..." \
        --api_key "xxx" \
        --base_url "http://wanqing.internal/api/agent/v1/apps" \
        --model "app-wcy0kf-xxx"
"""

import os
import json
import argparse
from datetime import datetime
from pathlib import Path
from openai import OpenAI

# ä»»åŠ¡ç”Ÿæˆçš„System Prompt
TASK_GENERATION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æµ‹è¯•å·¥ç¨‹å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç½‘ç«™/åº”ç”¨ç”Ÿæˆå…¨é¢çš„åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ã€‚

ç»™å®šä¸€ä¸ªç½‘ç«™çš„åŠŸèƒ½æè¿°ï¼Œä½ éœ€è¦ç”Ÿæˆ5-10ä¸ªç‹¬ç«‹çš„æµ‹è¯•ç”¨ä¾‹ã€‚æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹åº”è¯¥ï¼š
1. æµ‹è¯•ä¸€ä¸ªæ˜ç¡®çš„åŠŸèƒ½ç‚¹
2. æœ‰æ¸…æ™°çš„æ“ä½œæ­¥éª¤æè¿°
3. æœ‰æ˜ç¡®çš„æœŸæœ›ç»“æœ

**æµ‹è¯•ç”¨ä¾‹ç±»å‹**ï¼ˆå°½é‡è¦†ç›–ï¼‰ï¼š
- æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•ï¼ˆä¸»è¦ä¸šåŠ¡é€»è¾‘ï¼‰
- ç”¨æˆ·äº¤äº’æµ‹è¯•ï¼ˆæŒ‰é’®ã€è¡¨å•ã€å¯¼èˆªï¼‰
- æ•°æ®å±•ç¤ºæµ‹è¯•ï¼ˆå†…å®¹æ¸²æŸ“ã€æ•°æ®æ­£ç¡®æ€§ï¼‰
- è¾¹ç•Œæ¡ä»¶æµ‹è¯•ï¼ˆç©ºè¾“å…¥ã€é”™è¯¯è¾“å…¥ï¼‰
- UIä¸€è‡´æ€§æµ‹è¯•ï¼ˆæ ·å¼ã€å¸ƒå±€ã€å“åº”å¼ï¼‰

**è¾“å‡ºæ ¼å¼**ï¼ˆä¸¥æ ¼éµå®ˆJSONæ ¼å¼ï¼‰ï¼š
```json
{
  "test_cases": [
    {
      "id": 1,
      "task": "æµ‹è¯•ä»»åŠ¡çš„æè¿°ï¼ˆç”¨æˆ·éœ€è¦åšä»€ä¹ˆï¼‰",
      "expected_result": "æœŸæœ›çœ‹åˆ°çš„ç»“æœï¼ˆåº”è¯¥å‘ç”Ÿä»€ä¹ˆï¼‰",
      "priority": "high/medium/low",
      "category": "åŠŸèƒ½æµ‹è¯•/äº¤äº’æµ‹è¯•/æ•°æ®æµ‹è¯•/è¾¹ç•Œæµ‹è¯•/UIæµ‹è¯•"
    }
  ]
}
```

**é‡è¦è§„åˆ™**ï¼š
- æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹åº”è¯¥æ˜¯**ç‹¬ç«‹çš„**ï¼Œä¸ä¾èµ–å…¶ä»–æµ‹è¯•ç”¨ä¾‹
- ä¼˜å…ˆæµ‹è¯•æ ¸å¿ƒåŠŸèƒ½
- æµ‹è¯•æè¿°è¦å…·ä½“ï¼Œé¿å…æ¨¡ç³Š
- æœŸæœ›ç»“æœè¦å¯éªŒè¯
- ç”Ÿæˆ5-10ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆæ ¹æ®åº”ç”¨å¤æ‚åº¦ï¼‰
"""


def generate_test_cases(instruction: str, client: OpenAI, model: str) -> list:
    """
    æ ¹æ®ç½‘ç«™æŒ‡ä»¤ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹åˆ—è¡¨

    Args:
        instruction: ç½‘ç«™åŠŸèƒ½æè¿°
        client: OpenAIå®¢æˆ·ç«¯
        model: æ¨¡å‹åç§°

    Returns:
        æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
    """
    print("=" * 70)
    print("é˜¶æ®µ1ï¼šç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
    print("=" * 70)

    prompt = f"""
è¯·ä¸ºä»¥ä¸‹ç½‘ç«™/åº”ç”¨ç”Ÿæˆå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ï¼š

{instruction}

è¯·ç”Ÿæˆ5-10ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–ä¸»è¦åŠŸèƒ½ç‚¹ã€‚è¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ã€‚
"""

    try:
        print("\næ­£åœ¨è°ƒç”¨LLMç”Ÿæˆæµ‹è¯•ç”¨ä¾‹...")
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": TASK_GENERATION_PROMPT},
                {"role": "user", "content": prompt}
            ]
        )

        content = response.choices[0].message.content

        # æå–JSONéƒ¨åˆ†ï¼ˆå¦‚æœæ¨¡å‹è¿”å›äº†markdownæ ¼å¼ï¼‰
        if "```json" in content:
            json_start = content.find("```json") + 7
            json_end = content.find("```", json_start)
            content = content[json_start:json_end].strip()
        elif "```" in content:
            json_start = content.find("```") + 3
            json_end = content.find("```", json_start)
            content = content[json_start:json_end].strip()

        data = json.loads(content)
        test_cases = data.get("test_cases", [])

        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼š")
        print("-" * 70)
        for tc in test_cases:
            print(f"[{tc['id']}] {tc['task']}")
            print(f"    æœŸæœ›: {tc['expected_result'][:60]}...")
            print(f"    ä¼˜å…ˆçº§: {tc['priority']} | ç±»åˆ«: {tc['category']}")
            print()

        return test_cases

    except Exception as e:
        print(f"âŒ ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
        print(f"åŸå§‹å“åº”: {content if 'content' in locals() else 'N/A'}")
        return []


def run_single_test(test_case: dict, url: str, api_key: str, base_url: str,
                   model: str, output_dir: str, test_index: int) -> dict:
    """
    æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹

    Args:
        test_case: æµ‹è¯•ç”¨ä¾‹å­—å…¸
        url: è¢«æµ‹è¯•çš„ç½‘ç«™URL
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        model: æ¨¡å‹åç§°
        output_dir: è¾“å‡ºç›®å½•
        test_index: æµ‹è¯•åºå·

    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    print("\n" + "=" * 70)
    print(f"æ‰§è¡Œæµ‹è¯• {test_index}: {test_case['task']}")
    print("=" * 70)

    # åˆ›å»ºå­ç›®å½•
    test_output_dir = os.path.join(output_dir, f"test_{test_index:02d}")
    os.makedirs(test_output_dir, exist_ok=True)

    # è°ƒç”¨è¯„ä¼°è„šæœ¬
    import subprocess

    cmd = [
        "python3", "eval_single_website_openai.py",
        "--url", url,
        "--task", test_case['task'],
        "--expected", test_case['expected_result'],
        "--api_key", api_key,
        "--base_url", base_url,
        "--model", model,
        "--output_dir", test_output_dir,
        "--max_iter", "15",
        "--headless"
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

        # è¯»å–ç»“æœ
        result_file = os.path.join(test_output_dir, "result.json")
        if os.path.exists(result_file):
            with open(result_file, 'r', encoding='utf-8') as f:
                eval_result = json.load(f)

            test_result = {
                "test_case": test_case,
                "result": eval_result.get("result", "UNKNOWN"),
                "iterations": eval_result.get("iterations", 0),
                "timestamp": eval_result.get("timestamp", ""),
                "output_dir": test_output_dir,
                "status": "completed"
            }
        else:
            test_result = {
                "test_case": test_case,
                "result": "ERROR",
                "error": "Result file not found",
                "status": "failed"
            }

    except subprocess.TimeoutExpired:
        test_result = {
            "test_case": test_case,
            "result": "TIMEOUT",
            "error": "Execution timeout (300s)",
            "status": "timeout"
        }
    except Exception as e:
        test_result = {
            "test_case": test_case,
            "result": "ERROR",
            "error": str(e),
            "status": "failed"
        }

    # æ‰“å°ç»“æœ
    result_emoji = {
        "YES": "âœ…",
        "PARTIAL": "âš ï¸",
        "NO": "âŒ",
        "TIMEOUT": "â±ï¸",
        "ERROR": "ğŸ”¥",
        "UNKNOWN": "â“"
    }
    emoji = result_emoji.get(test_result['result'], "â“")
    print(f"\n{emoji} ç»“æœ: {test_result['result']}")

    return test_result


def generate_report(results: list, output_dir: str, instruction: str):
    """
    ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š

    Args:
        results: æµ‹è¯•ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        instruction: åŸå§‹ç½‘ç«™æŒ‡ä»¤
    """
    # ç»Ÿè®¡
    total = len(results)
    yes_count = sum(1 for r in results if r.get('result') == 'YES')
    partial_count = sum(1 for r in results if r.get('result') == 'PARTIAL')
    no_count = sum(1 for r in results if r.get('result') == 'NO')
    error_count = sum(1 for r in results if r.get('result') in ['ERROR', 'TIMEOUT', 'UNKNOWN'])

    accuracy = (yes_count / total * 100) if total > 0 else 0

    report = {
        "summary": {
            "instruction": instruction,
            "total_tests": total,
            "yes": yes_count,
            "partial": partial_count,
            "no": no_count,
            "error": error_count,
            "accuracy": round(accuracy, 2),
            "pass_rate": round((yes_count + partial_count) / total * 100, 2) if total > 0 else 0,
            "timestamp": datetime.now().isoformat()
        },
        "detailed_results": results
    }

    # ä¿å­˜JSONæŠ¥å‘Š
    report_file = os.path.join(output_dir, "test_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_report = f"""# è‡ªåŠ¨æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ‘˜è¦

**æµ‹è¯•å¯¹è±¡**: {instruction[:100]}...
**æµ‹è¯•æ—¶é—´**: {report['summary']['timestamp']}
**æ€»æµ‹è¯•æ•°**: {total}

### ç»“æœç»Ÿè®¡

| ç»“æœ | æ•°é‡ | ç™¾åˆ†æ¯” |
|------|------|--------|
| âœ… YES | {yes_count} | {yes_count/total*100:.1f}% |
| âš ï¸ PARTIAL | {partial_count} | {partial_count/total*100:.1f}% |
| âŒ NO | {no_count} | {no_count/total*100:.1f}% |
| ğŸ”¥ ERROR | {error_count} | {error_count/total*100:.1f}% |

**å‡†ç¡®ç‡**: {accuracy:.2f}% (ä»…YES)
**é€šè¿‡ç‡**: {report['summary']['pass_rate']:.2f}% (YES + PARTIAL)

## è¯¦ç»†ç»“æœ

"""

    for i, result in enumerate(results, 1):
        tc = result['test_case']
        emoji = {"YES": "âœ…", "PARTIAL": "âš ï¸", "NO": "âŒ"}.get(result.get('result'), "â“")

        md_report += f"""
### æµ‹è¯• {i}: {tc['task']}

- **ç»“æœ**: {emoji} {result.get('result', 'UNKNOWN')}
- **æœŸæœ›**: {tc['expected_result']}
- **ä¼˜å…ˆçº§**: {tc.get('priority', 'N/A')}
- **ç±»åˆ«**: {tc.get('category', 'N/A')}
- **è¿­ä»£æ¬¡æ•°**: {result.get('iterations', 'N/A')}
- **è¾“å‡ºç›®å½•**: `{result.get('output_dir', 'N/A')}`

---
"""

    md_file = os.path.join(output_dir, "test_report.md")
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_report)

    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)
    print(f"æ€»æµ‹è¯•æ•°: {total}")
    print(f"âœ… YES:     {yes_count} ({yes_count/total*100:.1f}%)")
    print(f"âš ï¸ PARTIAL: {partial_count} ({partial_count/total*100:.1f}%)")
    print(f"âŒ NO:      {no_count} ({no_count/total*100:.1f}%)")
    print(f"ğŸ”¥ ERROR:   {error_count} ({error_count/total*100:.1f}%)")
    print(f"\nå‡†ç¡®ç‡: {accuracy:.2f}%")
    print(f"é€šè¿‡ç‡: {report['summary']['pass_rate']:.2f}%")
    print(f"\næŠ¥å‘Šå·²ä¿å­˜è‡³: {output_dir}/test_report.md")


def main():
    parser = argparse.ArgumentParser(description="è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¹¶æ‰§è¡Œè¯„ä¼°")
    parser.add_argument("--url", required=True, help="è¢«æµ‹è¯•ç½‘ç«™çš„URL")
    parser.add_argument("--instruction", required=True, help="ç½‘ç«™åŠŸèƒ½æè¿°")
    parser.add_argument("--api_key", required=True, help="APIå¯†é’¥")
    parser.add_argument("--base_url", required=True, help="APIåŸºç¡€URL")
    parser.add_argument("--model", required=True, help="æ¨¡å‹åç§°")
    parser.add_argument("--output_dir", default="auto_test_results", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--skip_generation", action="store_true",
                       help="è·³è¿‡æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆï¼Œä½¿ç”¨å·²æœ‰çš„test_cases.json")

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)

    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    client = OpenAI(api_key=args.api_key, base_url=args.base_url)

    # é˜¶æ®µ1ï¼šç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
    test_cases_file = os.path.join(output_dir, "test_cases.json")

    if args.skip_generation and os.path.exists(test_cases_file):
        print(f"ä½¿ç”¨å·²æœ‰çš„æµ‹è¯•ç”¨ä¾‹: {test_cases_file}")
        with open(test_cases_file, 'r', encoding='utf-8') as f:
            test_cases = json.load(f)
    else:
        test_cases = generate_test_cases(args.instruction, client, args.model)

        if not test_cases:
            print("âŒ æ— æ³•ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œé€€å‡º")
            return

        # ä¿å­˜æµ‹è¯•ç”¨ä¾‹
        with open(test_cases_file, 'w', encoding='utf-8') as f:
            json.dump(test_cases, f, indent=2, ensure_ascii=False)
        print(f"\næµ‹è¯•ç”¨ä¾‹å·²ä¿å­˜è‡³: {test_cases_file}")

    # é˜¶æ®µ2ï¼šæ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
    print("\n" + "=" * 70)
    print("é˜¶æ®µ2ï¼šé€ä¸ªæ‰§è¡Œæµ‹è¯•ç”¨ä¾‹")
    print("=" * 70)

    results = []
    for i, test_case in enumerate(test_cases, 1):
        result = run_single_test(
            test_case=test_case,
            url=args.url,
            api_key=args.api_key,
            base_url=args.base_url,
            model=args.model,
            output_dir=output_dir,
            test_index=i
        )
        results.append(result)

        # ä¿å­˜ä¸­é—´ç»“æœï¼ˆé˜²æ­¢ä¸­æ–­ï¼‰
        with open(os.path.join(output_dir, "results_intermediate.json"), 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results, output_dir, args.instruction)


if __name__ == "__main__":
    main()
